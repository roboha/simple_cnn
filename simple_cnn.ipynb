{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/scripts/data')\n",
    "subprocess.Popen(['tensorboard', '--logdir=../tb_logs'])\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 16\n",
    "#display_step = 10\n",
    "num_epochs = 100\n",
    "\n",
    "# Network Parameters\n",
    "edgelength = 64\n",
    "#num_pixels = edgelength * edgelength\n",
    "num_chans = 4\n",
    "#num_input = num_pixels * num_chans # MNIST data input (img shape: 28*28)\n",
    "num_classes = 4 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.65 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, edgelength, edgelength, num_chans])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    # x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv4 = maxpool2d(conv4, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv5 = conv2d(conv4, weights['wc5'], biases['bc5'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv5 = maxpool2d(conv5, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv5, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, num_chans, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wc4': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
    "    'wc5': tf.Variable(tf.random_normal([3, 3, 256, 512])),\n",
    "    \n",
    "    'wd1': tf.Variable(tf.random_normal([2*2*512, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bc3': tf.Variable(tf.random_normal([128])),\n",
    "    'bc4': tf.Variable(tf.random_normal([256])),\n",
    "    'bc5': tf.Variable(tf.random_normal([512])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/scripts/tb_logs/train/1')\n",
    "test_writer = tf.summary.FileWriter('/scripts/tb_logs/test/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(xloc, yloc, p):\n",
    "    X_file = pickle.load( open(xloc, \"rb\" ))\n",
    "    y_file = pickle.load( open(yloc, \"rb\" ))\n",
    "    \n",
    "    y_file = np.array(y_file)-1\n",
    "    n_values = np.max(y_file)+1\n",
    "    y_file = np.eye(n_values)[y_file]\n",
    "    \n",
    "    c = list(zip(X_file, y_file))\n",
    "    random.shuffle(c)\n",
    "    \n",
    "    X_file, y_file = zip(*c)\n",
    "    \n",
    "    number = int(len(y_file) * p)\n",
    "    \n",
    "    X_file_tr = X_file[:number]\n",
    "    y_file_tr = y_file[:number]\n",
    "    \n",
    "    X_file_te = X_file[number:]\n",
    "    y_file_te = y_file[number:]    \n",
    "    \n",
    "    return X_file_tr, y_file_tr, X_file_te, y_file_te\n",
    "\n",
    "def generate_batches(Xs, ys, bs):\n",
    "    #X_file = pickle.load(open(\"images.pkl\", \"rb\" ))\n",
    "    #y_file = pickle.load(open(\"classes.pkl\", \"rb\" ))\n",
    "    \n",
    "    X_batches = []\n",
    "    y_batches = []    \n",
    "    Xss = []\n",
    "    yss = []\n",
    "    \n",
    "    for i, x in enumerate(Xs):\n",
    "        S = gdal.Open(x)\n",
    "        A = S.ReadAsArray()\n",
    "        \n",
    "        where_are_NaNs = np.isnan(A)\n",
    "        A[where_are_NaNs] = -40.\n",
    "        \n",
    "        #print(np.min(A))\n",
    "            \n",
    "        A = np.transpose(A)\n",
    "        y = ys[i]\n",
    "        \n",
    "        Xss.append(A)\n",
    "        yss.append(y)\n",
    "        \n",
    "        if (i % bs == 0) and (i != 0):\n",
    "            X_batches.append(Xss)\n",
    "            y_batches.append(yss)\n",
    "            \n",
    "            Xss = []\n",
    "            yss = []\n",
    "            \n",
    "    X_batches[0] = X_batches[0][:-1]\n",
    "    y_batches[0] = y_batches[0][:-1]\n",
    "    return X_batches, y_batches\n",
    "\n",
    "def generate_test(Xs, ys):\n",
    "    Xss = []\n",
    "    yss = []\n",
    "    for i, X in enumerate(Xs):\n",
    "        S = gdal.Open(X)\n",
    "        A = S.ReadAsArray()\n",
    "        where_are_NaNs = np.isnan(A)\n",
    "        A[where_are_NaNs] = -40.\n",
    "        \n",
    "        #print(np.min(A))\n",
    "        A = np.transpose(A)\n",
    "        Xss.append(A)\n",
    "        yss.append(ys[i])\n",
    "    return Xss, yss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./sumatra_tiles')\n",
    "train_test_ratio = 0.85\n",
    "xtr, ytr, xte, yte = train_test_split('/scripts/data/pythondata/images.pkl', '/scripts/data/pythondata/classes.pkl', train_test_ratio)\n",
    "X_batches, y_batches = generate_batches(xtr, ytr, batch_size)\n",
    "X_b, y_b = generate_batches(xtr, ytr, batch_size)\n",
    "xte, yte = generate_test(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1155619456.0000, Training Accuracy= 0.750\n",
      "Testing Accuracy:\n",
      "0.616564\n",
      "Step 2, Minibatch Loss= 359117056.0000, Training Accuracy= 0.688\n",
      "Testing Accuracy:\n",
      "0.754601\n",
      "Step 3, Minibatch Loss= 109623712.0000, Training Accuracy= 0.750\n",
      "Testing Accuracy:\n",
      "0.730061\n",
      "Step 4, Minibatch Loss= 68466520.0000, Training Accuracy= 0.750\n",
      "Testing Accuracy:\n",
      "0.723926\n",
      "Step 5, Minibatch Loss= 31327096.0000, Training Accuracy= 0.812\n",
      "Testing Accuracy:\n",
      "0.739264\n",
      "Step 6, Minibatch Loss= 6303688.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.757669\n",
      "Step 7, Minibatch Loss= 5654132.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.748466\n",
      "Step 8, Minibatch Loss= 13038956.0000, Training Accuracy= 0.875\n",
      "Testing Accuracy:\n",
      "0.766871\n",
      "Step 9, Minibatch Loss= 61984.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.769939\n",
      "Step 10, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.809816\n",
      "Step 11, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.788344\n",
      "Step 12, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.803681\n",
      "Step 13, Minibatch Loss= 1223224.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.791411\n",
      "Step 14, Minibatch Loss= 6621834.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.797546\n",
      "Step 15, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.809816\n",
      "Step 16, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.794479\n",
      "Step 17, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.797546\n",
      "Step 18, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.785276\n",
      "Step 19, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.812883\n",
      "Step 20, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.788344\n",
      "Step 21, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.809816\n",
      "Step 22, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 23, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.825153\n",
      "Step 24, Minibatch Loss= 1267900.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.769939\n",
      "Step 25, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.800614\n",
      "Step 26, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 27, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.812883\n",
      "Step 28, Minibatch Loss= 5772770.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.779141\n",
      "Step 29, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.809816\n",
      "Step 30, Minibatch Loss= 7061958.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.819018\n",
      "Step 31, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.769939\n",
      "Step 32, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.785276\n",
      "Step 33, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.800614\n",
      "Step 34, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.794479\n",
      "Step 35, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.803681\n",
      "Step 36, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 37, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.788344\n",
      "Step 38, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.819018\n",
      "Step 39, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.822086\n",
      "Step 40, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.831288\n",
      "Step 41, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.819018\n",
      "Step 42, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.815951\n",
      "Step 43, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.812883\n",
      "Step 44, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.794479\n",
      "Step 45, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.797546\n",
      "Step 46, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.837423\n",
      "Step 47, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.825153\n",
      "Step 48, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.812883\n",
      "Step 49, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.800614\n",
      "Step 50, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.800614\n",
      "Step 51, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.831288\n",
      "Step 52, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.815951\n",
      "Step 53, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.794479\n",
      "Step 54, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.855828\n",
      "Step 55, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.788344\n",
      "Step 56, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.852761\n",
      "Step 57, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.825153\n",
      "Step 58, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.803681\n",
      "Step 59, Minibatch Loss= 1107230.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.779141\n",
      "Step 60, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.828221\n",
      "Step 61, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.779141\n",
      "Step 62, Minibatch Loss= 4189178.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.791411\n",
      "Step 63, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.831288\n",
      "Step 64, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.834356\n",
      "Step 65, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.803681\n",
      "Step 66, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.834356\n",
      "Step 67, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.825153\n",
      "Step 68, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.849693\n",
      "Step 69, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 70, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.834356\n",
      "Step 71, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.822086\n",
      "Step 72, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 73, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.819018\n",
      "Step 74, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.819018\n",
      "Step 75, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.794479\n",
      "Step 76, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.800614\n",
      "Step 77, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.849693\n",
      "Step 78, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 79, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.846626\n",
      "Step 80, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.849693\n",
      "Step 81, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.861963\n",
      "Step 82, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.809816\n",
      "Step 83, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.822086\n",
      "Step 84, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.812883\n",
      "Step 85, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.828221\n",
      "Step 86, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.825153\n",
      "Step 87, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.806748\n",
      "Step 88, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.773006\n",
      "Step 89, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.834356\n",
      "Step 90, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.822086\n",
      "Step 91, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.837423\n",
      "Step 92, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.822086\n",
      "Step 93, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.812883\n",
      "Step 94, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.865031\n",
      "Step 95, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.858896\n",
      "Step 96, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846626\n",
      "Step 97, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.871166\n",
      "Step 98, Minibatch Loss= 1780264.0000, Training Accuracy= 0.938\n",
      "Testing Accuracy:\n",
      "0.800614\n",
      "Step 99, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.815951\n",
      "Step 100, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Testing Accuracy:\n",
      "0.849693\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "# step = 0\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(1, num_epochs + 1):\n",
    "        for i, xb in enumerate(X_b):\n",
    "            yb = y_b[i]\n",
    "            #print(np.min(np.array(xb)))\n",
    "            sess.run(train_op, feed_dict={X: xb, Y: yb, keep_prob: dropout})            \n",
    "            \n",
    "        if e % display_step == 0 or e == 1:\n",
    "            summary, loss, acc = sess.run([merged, loss_op, accuracy], feed_dict={X: xb,\n",
    "                                                                 Y: yb,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            train_writer.add_summary(summary, e)\n",
    "\n",
    "            print(\"Step \" + str(e) + \", Minibatch Loss= \" + \\\n",
    "                    \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    \"{:.3f}\".format(acc))\n",
    "\n",
    "            # Calculate accuracy for 256 MNIST test images\n",
    "            print(\"Testing Accuracy:\")\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict={X: xte,\n",
    "                                      Y: yte,\n",
    "                                      keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, e)\n",
    "            print(acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
