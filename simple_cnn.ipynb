{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/exch/scripts/sampler')\n",
    "from Sampler import batchload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORKDIR = '/exch/gdrive/oilp_rubber'\n",
    "SESSIONDIR = '/exch/scripts/tf_sess'\n",
    "boarddir_train = '/exch/scripts/tb_logs/train/310'\n",
    "boarddir_test = '/exch/scripts/tb_logs/test/310'\n",
    "os.chdir(WORKDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#subprocess.Popen(['tensorboard', '--logdir=/exch/tb_logs', '--port=6006'])\n",
    "learning_rate = 0.0001\n",
    "num_steps = 500\n",
    "batch_size = 8\n",
    "#display_step = 10\n",
    "num_epochs = 10000\n",
    "\n",
    "# Network Parameters\n",
    "edgelength = 32\n",
    "#num_pixels = edgelength * edgelength\n",
    "num_chans = 6\n",
    "#num_input = num_pixels * num_chans # MNIST data input (img shape: 28*28)\n",
    "num_classes = 5 # MNIST total classes (0-9 digits)\n",
    "dropout = .85 # Dropout, probability to keep units\n",
    "fac=2\n",
    "\n",
    "# tf Graph input\n",
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(tf.float32, [None, edgelength, edgelength, num_chans])\n",
    "    tf.summary.image('input_images', X[:, :, :, 0:3][:, :, :, ::-1], max_outputs=4)\n",
    "    Y = tf.placeholder(tf.int32, [None, num_classes])#num_classes])\n",
    "    #Y_oh = tf.one_hot(Y, 8)\n",
    "    keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "    phase_t = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1, phase=0):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    x = tf.layers.batch_normalization(x, training=phase)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout, phase_t):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    # x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'], phase=phase_t)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'], phase=phase_t)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'], phase=phase_t)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'], phase=phase_t)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv4 = maxpool2d(conv4, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    #conv5 = conv2d(conv4, weights['wc5'], biases['bc5'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    #conv5 = maxpool2d(conv5, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv4, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, num_chans, int(32*fac)])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, int(32*fac), int(64*fac)])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, int(64*fac), int(128*fac)])),\n",
    "    'wc4': tf.Variable(tf.random_normal([3, 3, int(128*fac), int(256*fac)])),\n",
    "    #'wc5': tf.Variable(tf.random_normal([3, 3, 256, 512])),\n",
    "    \n",
    "    'wd1': tf.Variable(tf.random_normal([2*2*int(256*fac), int(512*fac)])),\n",
    "    'out': tf.Variable(tf.random_normal([int(512*fac), num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([int(32*fac)])),\n",
    "    'bc2': tf.Variable(tf.random_normal([int(64*fac)])),\n",
    "    'bc3': tf.Variable(tf.random_normal([int(128*fac)])),\n",
    "    'bc4': tf.Variable(tf.random_normal([int(256*fac)])),\n",
    "    #'bc5': tf.Variable(tf.random_normal([512])),\n",
    "    'bd1': tf.Variable(tf.random_normal([int(512*fac)])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob, phase_t)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "#Notice - we introduce our L2 here\n",
    "vars_to_regul   = tf.trainable_variables() \n",
    "lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars_to_regul\n",
    "                    if 'bias' not in v.name ]) * 0.001\n",
    "\n",
    "#  loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#    out_layer, tf_train_labels) +\n",
    "#    beta*tf.nn.l2_loss(hidden_weights) +\n",
    "#    beta*tf.nn.l2_loss(hidden_biases) +\n",
    "#    beta*tf.nn.l2_loss(out_weights) +\n",
    "#    beta*tf.nn.l2_loss(out_biases)))\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y)) + lossL2\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    #train_op = optimizer.minimize(loss)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "\n",
    "######train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(boarddir_train)\n",
    "test_writer = tf.summary.FileWriter(boarddir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batches, y_batches, xte, yte, MNs, SDs = batchload(WORKDIR, 0.8, num_classes, batch_size=8, edgelength=32, centr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xte = xte[:32]\n",
    "#yte = np.expand_dims(yte[:8],1)\n",
    "a = np.array(yte[:32]-1)\n",
    "b = np.zeros((32, num_classes))\n",
    "b[np.arange(32), a] = 1\n",
    "yte = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "# step = 0\n",
    "display_step = 5\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(1, num_epochs + 1):\n",
    "        for i, xb in enumerate(X_batches):\n",
    "            yb = y_batches[i]\n",
    "            #yb = np.expand_dims(yb,1)            \n",
    "            a = np.array(yb-1)\n",
    "            b = np.zeros((8, num_classes))\n",
    "            b[np.arange(8), a] = 1\n",
    "            yb = b\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X: xb, Y: yb, keep_prob: dropout, phase_t: 1}) \n",
    "            #print(i)\n",
    "            \n",
    "        if e % display_step == 0 or e == 1:\n",
    "            summary, loss, acc = sess.run([merged, loss_op, accuracy], feed_dict={X: xb,\n",
    "                                                                 Y: yb,\n",
    "                                                                 keep_prob: 1.0,\n",
    "                                                                 phase_t: 0})\n",
    "            train_writer.add_summary(summary, e)\n",
    "\n",
    "            print(\"Step \" + str(e) + \", Minibatch Loss= \" + \\\n",
    "                    \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    \"{:.3f}\".format(acc))\n",
    "\n",
    "            # Calculate accuracy for 256 MNIST test images\n",
    "            print(\"Testing Accuracy:\")\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict={X: xte,\n",
    "                                      Y: yte,\n",
    "                                      keep_prob: 1.0,\n",
    "                                      phase_t: 0})\n",
    "            test_writer.add_summary(summary, e)\n",
    "            print(acc)\n",
    "            \n",
    "            save_path = saver.save(sess, SESSIONDIR+\"/model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = gdal.Open('/exch/gdrive/newhm_again.tif')\n",
    "Xr = S.ReadAsArray()\n",
    "where_are_NaNs = np.isnan(Xr)\n",
    "Xr[where_are_NaNs] = -40.\n",
    "\n",
    "Z = [(Xr[i,:,:] - MNs[i])/SDs[i] for i in range(len(MNs))]\n",
    "Z = np.array(Z)\n",
    "Z = np.transpose(Z)\n",
    "#M = np.zeros(Xr[:,:,0].shape)\n",
    "\n",
    "M = np.zeros(Z[:,:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "yb = y_batches[2]-1       \n",
    "a = np.array(yb)\n",
    "b = np.zeros((8, num_classes))\n",
    "b[np.arange(8), a] = 1\n",
    "yb = b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, SESSIONDIR+\"/model.ckpt\")\n",
    "    \n",
    "    for i in range(18,Z.shape[0]-18):\n",
    "        for j in range(18,Z.shape[1]-18):\n",
    "            #print(y)\n",
    "            classify_this = Z[i-int(edgelength/2):i+int(edgelength/2),j-int(edgelength/2):j+int(edgelength/2),:]\n",
    "            ct = np.expand_dims(classify_this, axis=0)           \n",
    "            y_hat = sess.run(prediction, feed_dict={X: ct, Y: yb, keep_prob:1.0, phase_t: 0})#np.expand_dims(yb[0], axis=0), keep_prob:1.0})\n",
    "            CLASS = np.argmax(y_hat)\n",
    "            M[i,j] = CLASS\n",
    "            #print(CLASS)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write output\n",
    "#S2 = gdal.Open('/data/Hdd1/CNNTEST.tif')\n",
    "S2 = S\n",
    "Xr2 = S2.ReadAsArray()\n",
    "driver = gdal.GetDriverByName('Gtiff')\n",
    "dstfile = WORKDIR+'/oil_rubber_map_3.tif'\n",
    "dataset2 = driver.Create(dstfile, S2.RasterXSize, S2.RasterYSize, 1, gdal.GDT_Float32)\n",
    "dataset2.SetGeoTransform(S2.GetGeoTransform())\n",
    "dataset2.SetProjection(S2.GetProjection())\n",
    "Mt = M.transpose()\n",
    "dataset2.GetRasterBand(1).WriteArray(Mt)\n",
    "dataset2.FlushCache()\n",
    "dataset2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
